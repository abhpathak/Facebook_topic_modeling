{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Importing Library and setting environment path\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "os.environ['SPARK_HOME'] = \"C:/Users/Abhinav/Downloads/spark-2.0.2-bin-hadoop2.7\"\n",
    "sys.path.append(\"C:/Users/Abhinav/Downloads/spark-2.0.2-bin-hadoop2.7/bin\")\n",
    "sys.path.append(\"C:/Users/Abhinav/Downloads/spark-2.0.2-bin-hadoop2.7/python\")\n",
    "sys.path.append(\"C:/Users/Abhinav/Downloads/spark-2.0.2-bin-hadoop2.7/python/pyspark\")\n",
    "sys.path.append(\"C:/Users/Abhinav/Downloads/spark-2.0.2-bin-hadoop2.7/python/lib\")\n",
    "sys.path.append(\"C:/Users/Abhinav/Downloads/spark-2.0.2-bin-hadoop2.7/python/lib/pyspark.zip\")\n",
    "sys.path.append(\"C:/Users/Abhinav/Downloads/spark-2.0.2-bin-hadoop2.7/python/lib/py4j-0.10.3-src.zip\")\n",
    "sys.path.append(\"C:/Program Files/Java/jre1.8.0_77/bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#website = \"nytimes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.clustering import LDA, LDAModel\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "\n",
    "data = sc.textFile(\"C:/Users/Abhinav/Google Drive/Big Data Project/Datasets/All combined/all_data.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing punctuations and numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def removePunctuation(text):\n",
    "    inter1 = re.sub('[^a-z| |]', '', text.strip().lower().encode('utf-8'))\n",
    "    inter2 = inter1.replace(\"\\'\", \"\")\n",
    "    return inter2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_sanspunct = data.map(removePunctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from stop_words import get_stop_words\n",
    "en_stop = get_stop_words('en')\n",
    "\n",
    "def removeStopWords(line):\n",
    "    tokens = tokenizer.tokenize(line)\n",
    "    words = [i for i in tokens if not i in en_stop]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_sansstopwords = data_sanspunct.map(removeStopWords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "p_stemmer = PorterStemmer()\n",
    "\n",
    "def stemmingWords(words):\n",
    "    stemmed_word = [p_stemmer.stem(i) for i in words]\n",
    "    return stemmed_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_sansstemmingwords = data_sansstopwords.map(stemmingWords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating document-term matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abhinav\\AppData\\Local\\Enthought\\Canopy\\User\\lib\\site-packages\\gensim\\utils.py:840: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "C:\\Users\\Abhinav\\AppData\\Local\\Enthought\\Canopy\\User\\lib\\site-packages\\gensim\\utils.py:1015: UserWarning: Pattern library is not installed, lemmatization won't be available.\n",
      "  warnings.warn(\"Pattern library is not installed, lemmatization won't be available.\")\n"
     ]
    }
   ],
   "source": [
    "import gensim \n",
    "from gensim import corpora, models\n",
    "\n",
    "def documentTermMatrix(texts):\n",
    "    dictionary = corpora.Dictionary(texts)\n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corpus = data_sansstemmingwords.map(lambda word:documentTermMatrix(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_list = data_sansstemmingwords.collect()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc_clean = [doc.split() for doc in word_list]\n",
    "dictionary = corpora.Dictionary(doc_clean)\n",
    "corpus = [dictionary.doc2bow(text) for text in doc_clean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim import corpora, models\n",
    "\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=30, id2word = dictionary, iterations=3, random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# select top 50 words for each of the 20 LDA topics\n",
    "top_words = [[str(word[0].encode('utf-8')) for word in ldamodel.show_topic(topicno, topn=50)] for topicno in range(ldamodel.num_topics)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: time know happen wasnut uthi stand let ever put noth\n",
      "Topic 1: just come two race still better ufollow ulanda includ expert\n",
      "Topic 2: player guy complet give comment field went victori michigan uniform\n",
      "Topic 3: right use chang doesnut late overal america anyth car usual\n",
      "Topic 4: get countri presid report second bring th highli either line\n",
      "Topic 5: u leagu slalom pick real thereu stop packer obvious bryant\n",
      "Topic 6: said game uth fan shot era life news uthatu winud\n",
      "Topic 7: ui start next made keep mani candid left bradi note\n",
      "Topic 8: year landau return believ offici four away uwhat washington although\n",
      "Topic 9: need take lot ball deal papadaki uhe live spent less\n",
      "Topic 10: like got came new injuri show issu defens roster uin\n",
      "Topic 11: one donut goal call coach young help eight wear wild\n",
      "Topic 12: man norman pat train turn cut sure goodel respect thought\n",
      "Topic 13: way well won miss twitter fact test place dak russia\n",
      "Topic 14: none never szew told home prix coupl due terribl uuch\n",
      "Topic 15: team see realli top s around count big ago part\n",
      "Topic 16: now week someth beat side dez cup fumbl half wednesday\n",
      "Topic 17: back career sport day organ pennsylvania club took tuesday total\n",
      "Topic 18: play itu point lead sign work sunday uit far watch\n",
      "Topic 19: even elect us best contract hope person decid three giant\n",
      "Topic 20: can much also ua lost democrat finish inform save check\n",
      "Topic 21: make tri mr score lose polic done problem qb power\n",
      "Topic 22: will didnut earnhardt canut pitcher ballot titl thank nice wonut\n",
      "Topic 23: go nfl final great alway famili name campaign perfect close\n",
      "Topic 24: look night million result recount mean hit machin differ anyon\n",
      "Topic 25: last win state clinton everi pass talk twin support ubut\n",
      "Topic 26: vote season want say think first world thing anoth littl\n",
      "Topic 27: good run nation footbal ium playoff sinc dalla matter least\n",
      "Topic 28: peopl thatu heu moir concuss media five form seem though\n",
      "Topic 29: trump cowboy uif may champion lebron move super polit probabl\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import  numpy as np\n",
    "# get all top 50 words in all 20 topics, as one large set\n",
    "all_words = set(itertools.chain.from_iterable(top_words))\n",
    "\n",
    "# for each topic, replace a word at a different index, to make it more interesting\n",
    "#replace_index = np.random.randint(0, 10, ldamodel.num_topics)\n",
    "\n",
    "replacements = []\n",
    "for topicno, words in enumerate(top_words):\n",
    "#    other_words = all_words.difference(words)\n",
    "#    replacement = np.random.choice(list(other_words))\n",
    "#    replacements.append((words[replace_index[topicno]], replacement))\n",
    "#    words[replace_index[topicno]] = replacement\n",
    "    print(\"Topic %i: %s\" % (topicno, ' '.join(words[:10])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['time',\n",
       "  'know',\n",
       "  'happen',\n",
       "  'wasnut',\n",
       "  'uthi',\n",
       "  'stand',\n",
       "  'let',\n",
       "  'ever',\n",
       "  'put',\n",
       "  'noth',\n",
       "  'gonna',\n",
       "  'actual',\n",
       "  'third',\n",
       "  'care',\n",
       "  'yard',\n",
       "  'target',\n",
       "  'everyth',\n",
       "  'elector',\n",
       "  'obama',\n",
       "  'heud',\n",
       "  'bowl',\n",
       "  'biggest',\n",
       "  'guess',\n",
       "  'reach',\n",
       "  'cook',\n",
       "  'knee',\n",
       "  'expand',\n",
       "  'father',\n",
       "  'prove',\n",
       "  'choos',\n",
       "  'correct',\n",
       "  'intern',\n",
       "  'comeback',\n",
       "  'wheel',\n",
       "  'began',\n",
       "  'educ',\n",
       "  'poor',\n",
       "  'panther',\n",
       "  'senat',\n",
       "  'youuv',\n",
       "  'grew',\n",
       "  'arduou',\n",
       "  'nhk',\n",
       "  'met',\n",
       "  'prepar',\n",
       "  'truth',\n",
       "  'shut',\n",
       "  'pile',\n",
       "  'citizen',\n",
       "  'intent'],\n",
       " ['just',\n",
       "  'come',\n",
       "  'two',\n",
       "  'race',\n",
       "  'still',\n",
       "  'better',\n",
       "  'ufollow',\n",
       "  'ulanda',\n",
       "  'includ',\n",
       "  'expert',\n",
       "  'hard',\n",
       "  'drop',\n",
       "  'minor',\n",
       "  'professor',\n",
       "  'troll',\n",
       "  'nascar',\n",
       "  'realiti',\n",
       "  'content',\n",
       "  'answer',\n",
       "  'ufriday',\n",
       "  'conspiraci',\n",
       "  'ucsup',\n",
       "  'uclittl',\n",
       "  'acknowledgedxalanda',\n",
       "  'everybodi',\n",
       "  'moment',\n",
       "  'son',\n",
       "  'definit',\n",
       "  'notic',\n",
       "  'school',\n",
       "  'neg',\n",
       "  'symptom',\n",
       "  'youngest',\n",
       "  'star',\n",
       "  'iull',\n",
       "  'hater',\n",
       "  'multipli',\n",
       "  'telephon',\n",
       "  'ueveri',\n",
       "  'golden',\n",
       "  'led',\n",
       "  'send',\n",
       "  'held',\n",
       "  'delay',\n",
       "  'minut',\n",
       "  'daniel',\n",
       "  'justifi',\n",
       "  'pack',\n",
       "  'wednesdayu',\n",
       "  'twogoal'],\n",
       " ['player',\n",
       "  'guy',\n",
       "  'complet',\n",
       "  'give',\n",
       "  'comment',\n",
       "  'field',\n",
       "  'went',\n",
       "  'victori',\n",
       "  'michigan',\n",
       "  'uniform',\n",
       "  'fall',\n",
       "  'enough',\n",
       "  'everyon',\n",
       "  'burn',\n",
       "  'job',\n",
       "  'caus',\n",
       "  'defend',\n",
       "  'question',\n",
       "  'mayb',\n",
       "  'perform',\n",
       "  'rate',\n",
       "  'tom',\n",
       "  'easi',\n",
       "  'front',\n",
       "  'hear',\n",
       "  'public',\n",
       "  'popular',\n",
       "  'dress',\n",
       "  'add',\n",
       "  'soon',\n",
       "  'open',\n",
       "  'ucnev',\n",
       "  'ucheck',\n",
       "  'justic',\n",
       "  'idiot',\n",
       "  'situat',\n",
       "  'tire',\n",
       "  'role',\n",
       "  'hour',\n",
       "  'york',\n",
       "  'list',\n",
       "  'thousand',\n",
       "  'listen',\n",
       "  'focus',\n",
       "  'octob',\n",
       "  'litterxath',\n",
       "  'collect',\n",
       "  'choic',\n",
       "  'uanaheim',\n",
       "  'order'],\n",
       " ['right',\n",
       "  'use',\n",
       "  'chang',\n",
       "  'doesnut',\n",
       "  'late',\n",
       "  'overal',\n",
       "  'america',\n",
       "  'anyth',\n",
       "  'car',\n",
       "  'usual',\n",
       "  'uand',\n",
       "  'minorleagu',\n",
       "  'despit',\n",
       "  'might',\n",
       "  'kiss',\n",
       "  'individu',\n",
       "  'men',\n",
       "  'ucitu',\n",
       "  'screw',\n",
       "  'mose',\n",
       "  'meyer',\n",
       "  'cop',\n",
       "  'benefit',\n",
       "  'nbc',\n",
       "  'homer',\n",
       "  'uwel',\n",
       "  'uch',\n",
       "  'girl',\n",
       "  'rememb',\n",
       "  'anymor',\n",
       "  'longev',\n",
       "  'vancouv',\n",
       "  'oh',\n",
       "  'north',\n",
       "  'enjoy',\n",
       "  'mate',\n",
       "  'appeal',\n",
       "  'suit',\n",
       "  'fourth',\n",
       "  'bout',\n",
       "  'discuss',\n",
       "  'closer',\n",
       "  'present',\n",
       "  'signific',\n",
       "  'entertain',\n",
       "  'crime',\n",
       "  'widespread',\n",
       "  'cardif',\n",
       "  'nicknam',\n",
       "  'variou'],\n",
       " ['get',\n",
       "  'countri',\n",
       "  'presid',\n",
       "  'report',\n",
       "  'second',\n",
       "  'bring',\n",
       "  'th',\n",
       "  'highli',\n",
       "  'either',\n",
       "  'line',\n",
       "  'cleveland',\n",
       "  'hand',\n",
       "  'deadlin',\n",
       "  'wonder',\n",
       "  'depart',\n",
       "  'pleas',\n",
       "  'quit',\n",
       "  'disciplin',\n",
       "  'recent',\n",
       "  'figur',\n",
       "  'friend',\n",
       "  'push',\n",
       "  'rest',\n",
       "  'patient',\n",
       "  'exist',\n",
       "  'allnba',\n",
       "  'heartbreak',\n",
       "  'build',\n",
       "  'handl',\n",
       "  'special',\n",
       "  'unot',\n",
       "  'attempt',\n",
       "  'doubt',\n",
       "  'absolut',\n",
       "  'busi',\n",
       "  'chicago',\n",
       "  'plu',\n",
       "  'crew',\n",
       "  'match',\n",
       "  'exhaust',\n",
       "  'eventu',\n",
       "  'monday',\n",
       "  'jimmi',\n",
       "  'along',\n",
       "  'slightli',\n",
       "  'evid',\n",
       "  'bare',\n",
       "  'keyston',\n",
       "  'method',\n",
       "  'loser'],\n",
       " ['u',\n",
       "  'leagu',\n",
       "  'slalom',\n",
       "  'pick',\n",
       "  'real',\n",
       "  'thereu',\n",
       "  'stop',\n",
       "  'packer',\n",
       "  'obvious',\n",
       "  'bryant',\n",
       "  'nfc',\n",
       "  'jose',\n",
       "  'uvirtu',\n",
       "  'wrong',\n",
       "  'flag',\n",
       "  'tie',\n",
       "  'administr',\n",
       "  'spoke',\n",
       "  'couldnut',\n",
       "  'legend',\n",
       "  'umor',\n",
       "  'berardino',\n",
       "  'dumb',\n",
       "  'local',\n",
       "  'repeat',\n",
       "  'whatu',\n",
       "  'uwith',\n",
       "  'seasonud',\n",
       "  'scoredxaeven',\n",
       "  'umoir',\n",
       "  'superg',\n",
       "  'murren',\n",
       "  'respons',\n",
       "  'theyul',\n",
       "  'shoot',\n",
       "  'chunk',\n",
       "  'committe',\n",
       "  'suspect',\n",
       "  'udez',\n",
       "  'iud',\n",
       "  'triumph',\n",
       "  'psi',\n",
       "  'fraud',\n",
       "  'matchup',\n",
       "  'sabotag',\n",
       "  'ucanut',\n",
       "  'user',\n",
       "  'reid',\n",
       "  'audit',\n",
       "  'undo'],\n",
       " ['said',\n",
       "  'game',\n",
       "  'uth',\n",
       "  'fan',\n",
       "  'shot',\n",
       "  'era',\n",
       "  'life',\n",
       "  'news',\n",
       "  'uthatu',\n",
       "  'winud',\n",
       "  'ice',\n",
       "  'flyer',\n",
       "  'awesom',\n",
       "  'accus',\n",
       "  'theme',\n",
       "  'rim',\n",
       "  'knewud',\n",
       "  'client',\n",
       "  'evenkeeledud',\n",
       "  'suffer',\n",
       "  'healthi',\n",
       "  'teamu',\n",
       "  'seventh',\n",
       "  'lol',\n",
       "  'retir',\n",
       "  'uucat',\n",
       "  'fool',\n",
       "  'steve',\n",
       "  'bet',\n",
       "  'elud',\n",
       "  'determin',\n",
       "  'energi',\n",
       "  'utrump',\n",
       "  'elit',\n",
       "  'bottomsix',\n",
       "  'wr',\n",
       "  'except',\n",
       "  'enorm',\n",
       "  'remnant',\n",
       "  'memori',\n",
       "  'thanksgiv',\n",
       "  'longterm',\n",
       "  'zuckermanpark',\n",
       "  'dirti',\n",
       "  'slippingxainto',\n",
       "  'band',\n",
       "  'uwatch',\n",
       "  'employ',\n",
       "  'sorri',\n",
       "  'puck'],\n",
       " ['ui',\n",
       "  'start',\n",
       "  'next',\n",
       "  'made',\n",
       "  'keep',\n",
       "  'mani',\n",
       "  'candid',\n",
       "  'left',\n",
       "  'bradi',\n",
       "  'note',\n",
       "  'kaepernick',\n",
       "  'uno',\n",
       "  'uuci',\n",
       "  'agent',\n",
       "  'progress',\n",
       "  'attent',\n",
       "  'die',\n",
       "  'seven',\n",
       "  'tillman',\n",
       "  'action',\n",
       "  'wouldnut',\n",
       "  'colin',\n",
       "  'practic',\n",
       "  'throwback',\n",
       "  'cousin',\n",
       "  'floor',\n",
       "  'bucket',\n",
       "  'issel',\n",
       "  'rickrol',\n",
       "  'sadden',\n",
       "  'lineup',\n",
       "  'lossud',\n",
       "  'basebal',\n",
       "  'cast',\n",
       "  'unoffici',\n",
       "  'ran',\n",
       "  'crowd',\n",
       "  'class',\n",
       "  'gate',\n",
       "  'runnersup',\n",
       "  'mention',\n",
       "  'board',\n",
       "  'momentum',\n",
       "  'hold',\n",
       "  'quarterback',\n",
       "  'food',\n",
       "  'complaint',\n",
       "  'indic',\n",
       "  'engin',\n",
       "  'blow'],\n",
       " ['year',\n",
       "  'landau',\n",
       "  'return',\n",
       "  'believ',\n",
       "  'offici',\n",
       "  'four',\n",
       "  'away',\n",
       "  'uwhat',\n",
       "  'washington',\n",
       "  'although',\n",
       "  'celebr',\n",
       "  'gener',\n",
       "  'manag',\n",
       "  'marseil',\n",
       "  'staff',\n",
       "  'understand',\n",
       "  'fine',\n",
       "  'instead',\n",
       "  'ive',\n",
       "  'juliu',\n",
       "  'lie',\n",
       "  'tongueincheek',\n",
       "  'ulebronu',\n",
       "  'malon',\n",
       "  'share',\n",
       "  'condol',\n",
       "  'resign',\n",
       "  'hamper',\n",
       "  'air',\n",
       "  'medal',\n",
       "  'paid',\n",
       "  'offseason',\n",
       "  'creat',\n",
       "  'babi',\n",
       "  'schedul',\n",
       "  'deflateg',\n",
       "  'gone',\n",
       "  'pad',\n",
       "  'gutu',\n",
       "  'commentsxaucweur',\n",
       "  'combin',\n",
       "  'futur',\n",
       "  'lucki',\n",
       "  'provid',\n",
       "  'ucowboy',\n",
       "  'bradyu',\n",
       "  'kick',\n",
       "  'protect',\n",
       "  'reveal',\n",
       "  'locker'],\n",
       " ['need',\n",
       "  'take',\n",
       "  'lot',\n",
       "  'ball',\n",
       "  'deal',\n",
       "  'papadaki',\n",
       "  'uhe',\n",
       "  'live',\n",
       "  'spent',\n",
       "  'less',\n",
       "  'percent',\n",
       "  'nba',\n",
       "  'free',\n",
       "  'act',\n",
       "  'interview',\n",
       "  'virtu',\n",
       "  'rick',\n",
       "  'true',\n",
       "  'promis',\n",
       "  'articl',\n",
       "  'prime',\n",
       "  'letu',\n",
       "  'ualso',\n",
       "  'hous',\n",
       "  'qualifi',\n",
       "  'advantag',\n",
       "  'darlington',\n",
       "  'streak',\n",
       "  'comput',\n",
       "  'swiftli',\n",
       "  'worldrecord',\n",
       "  'beast',\n",
       "  'ncaa',\n",
       "  'rang',\n",
       "  'ap',\n",
       "  'jersey',\n",
       "  'tee',\n",
       "  'spread',\n",
       "  'relev',\n",
       "  'contain',\n",
       "  'melissa',\n",
       "  'glitch',\n",
       "  'weekit',\n",
       "  'battleground',\n",
       "  'jet',\n",
       "  'contribut',\n",
       "  'theyuv',\n",
       "  'didnt',\n",
       "  'uucmickey',\n",
       "  'earnhardtu'],\n",
       " ['like',\n",
       "  'got',\n",
       "  'came',\n",
       "  'new',\n",
       "  'injuri',\n",
       "  'show',\n",
       "  'issu',\n",
       "  'defens',\n",
       "  'roster',\n",
       "  'uin',\n",
       "  'yearold',\n",
       "  'leader',\n",
       "  'complain',\n",
       "  'base',\n",
       "  'liber',\n",
       "  'impress',\n",
       "  'island',\n",
       "  'intellig',\n",
       "  'ulebron',\n",
       "  'gave',\n",
       "  'stick',\n",
       "  'crash',\n",
       "  'awar',\n",
       "  'lindholm',\n",
       "  'decad',\n",
       "  'cheat',\n",
       "  'sportsu',\n",
       "  'medic',\n",
       "  'nobodi',\n",
       "  'pathet',\n",
       "  'hashtag',\n",
       "  'ukaepernick',\n",
       "  'bradley',\n",
       "  'worri',\n",
       "  'global',\n",
       "  'embarrass',\n",
       "  'rbi',\n",
       "  'hillaryclinton',\n",
       "  'wroteth',\n",
       "  'doubet',\n",
       "  'gold',\n",
       "  'scrutini',\n",
       "  'danielmcfadin',\n",
       "  'sent',\n",
       "  'goneour',\n",
       "  'overwhelm',\n",
       "  'weuv',\n",
       "  'possibleud',\n",
       "  'seamless',\n",
       "  'univers'],\n",
       " ['one',\n",
       "  'donut',\n",
       "  'goal',\n",
       "  'call',\n",
       "  'coach',\n",
       "  'young',\n",
       "  'help',\n",
       "  'eight',\n",
       "  'wear',\n",
       "  'wild',\n",
       "  'behind',\n",
       "  'colleg',\n",
       "  'cours',\n",
       "  'offic',\n",
       "  'posit',\n",
       "  'uitu',\n",
       "  'join',\n",
       "  'roger',\n",
       "  'commun',\n",
       "  'suggest',\n",
       "  'boy',\n",
       "  'pay',\n",
       "  'halderman',\n",
       "  'refer',\n",
       "  'starter',\n",
       "  'kosar',\n",
       "  'facet',\n",
       "  'kap',\n",
       "  'paper',\n",
       "  'doctor',\n",
       "  'equal',\n",
       "  'carolina',\n",
       "  'connect',\n",
       "  'pull',\n",
       "  'rig',\n",
       "  'fair',\n",
       "  'involv',\n",
       "  'rather',\n",
       "  'uwhil',\n",
       "  'uthen',\n",
       "  'udo',\n",
       "  'magney',\n",
       "  'youbi',\n",
       "  'corrupt',\n",
       "  'emerg',\n",
       "  'lack',\n",
       "  'directli',\n",
       "  'cost',\n",
       "  'scienc',\n",
       "  'yeah'],\n",
       " ['man',\n",
       "  'norman',\n",
       "  'pat',\n",
       "  'train',\n",
       "  'turn',\n",
       "  'cut',\n",
       "  'sure',\n",
       "  'goodel',\n",
       "  'respect',\n",
       "  'thought',\n",
       "  'find',\n",
       "  'becom',\n",
       "  'fort',\n",
       "  'age',\n",
       "  'ulook',\n",
       "  'hate',\n",
       "  'championship',\n",
       "  'uci',\n",
       "  'presidenti',\n",
       "  'uucit',\n",
       "  'walk',\n",
       "  'uthat',\n",
       "  'neither',\n",
       "  'readi',\n",
       "  'uucitu',\n",
       "  'competit',\n",
       "  'teresa',\n",
       "  'leaveud',\n",
       "  'xbb',\n",
       "  'gambl',\n",
       "  'virginia',\n",
       "  'trade',\n",
       "  'truli',\n",
       "  'throne',\n",
       "  'afternoonu',\n",
       "  'trigger',\n",
       "  'mara',\n",
       "  'automat',\n",
       "  'boo',\n",
       "  'uwow',\n",
       "  'preelect',\n",
       "  'forth',\n",
       "  'brutal',\n",
       "  'baton',\n",
       "  'uucium',\n",
       "  'veteran',\n",
       "  'nervou',\n",
       "  'penalti',\n",
       "  'ufor',\n",
       "  'whatsoev'],\n",
       " ['way',\n",
       "  'well',\n",
       "  'won',\n",
       "  'miss',\n",
       "  'twitter',\n",
       "  'fact',\n",
       "  'test',\n",
       "  'place',\n",
       "  'dak',\n",
       "  'russia',\n",
       "  'compani',\n",
       "  'appear',\n",
       "  'allstar',\n",
       "  'theyur',\n",
       "  'messag',\n",
       "  'dunk',\n",
       "  'ud',\n",
       "  'saw',\n",
       "  'corner',\n",
       "  'nbau',\n",
       "  'dale',\n",
       "  'rooki',\n",
       "  'sound',\n",
       "  'award',\n",
       "  'steal',\n",
       "  'ross',\n",
       "  'citi',\n",
       "  'ushiffrinu',\n",
       "  'compar',\n",
       "  'oppos',\n",
       "  'level',\n",
       "  'market',\n",
       "  'ahead',\n",
       "  'pool',\n",
       "  'cri',\n",
       "  'describ',\n",
       "  'etc',\n",
       "  'elliott',\n",
       "  'uoh',\n",
       "  'salari',\n",
       "  'uall',\n",
       "  'racism',\n",
       "  'stood',\n",
       "  'grassroot',\n",
       "  'k',\n",
       "  'excus',\n",
       "  'legitimaci',\n",
       "  'writerchick',\n",
       "  'ubefor',\n",
       "  'clown'],\n",
       " ['none',\n",
       "  'never',\n",
       "  'szew',\n",
       "  'told',\n",
       "  'home',\n",
       "  'prix',\n",
       "  'coupl',\n",
       "  'due',\n",
       "  'terribl',\n",
       "  'uuch',\n",
       "  'clutterbuck',\n",
       "  'must',\n",
       "  'udaniel',\n",
       "  'confidentud',\n",
       "  'twinsu',\n",
       "  'troublesom',\n",
       "  'bit',\n",
       "  'carud',\n",
       "  'uucther',\n",
       "  'serious',\n",
       "  'releas',\n",
       "  'eventud',\n",
       "  'uher',\n",
       "  'assur',\n",
       "  'surgeri',\n",
       "  'christma',\n",
       "  'presley',\n",
       "  'mickey',\n",
       "  'shame',\n",
       "  'wore',\n",
       "  'nielsen',\n",
       "  'foolish',\n",
       "  'ucweur',\n",
       "  'ucberni',\n",
       "  'nicholasmendola',\n",
       "  'canuck',\n",
       "  'freak',\n",
       "  'natur',\n",
       "  'bulli',\n",
       "  'ulov',\n",
       "  'propaganda',\n",
       "  'ahol',\n",
       "  'debat',\n",
       "  'pig',\n",
       "  'inequ',\n",
       "  'kickoff',\n",
       "  'uclook',\n",
       "  'denver',\n",
       "  'usound',\n",
       "  'arabia'],\n",
       " ['team',\n",
       "  'see',\n",
       "  'realli',\n",
       "  'top',\n",
       "  's',\n",
       "  'around',\n",
       "  'count',\n",
       "  'big',\n",
       "  'ago',\n",
       "  'part',\n",
       "  'petit',\n",
       "  'protest',\n",
       "  'straight',\n",
       "  'possibl',\n",
       "  'record',\n",
       "  'histori',\n",
       "  'number',\n",
       "  'pretti',\n",
       "  'seattl',\n",
       "  'system',\n",
       "  'udur',\n",
       "  'upud',\n",
       "  'post',\n",
       "  'kid',\n",
       "  'nov',\n",
       "  'cap',\n",
       "  'berni',\n",
       "  'period',\n",
       "  'wcoastfangirl',\n",
       "  'danger',\n",
       "  'uther',\n",
       "  'elvin',\n",
       "  'ad',\n",
       "  'astley',\n",
       "  'ucdeepli',\n",
       "  'forxahelp',\n",
       "  'theori',\n",
       "  'greatud',\n",
       "  'alon',\n",
       "  'string',\n",
       "  'unlik',\n",
       "  'amid',\n",
       "  'among',\n",
       "  'stewart',\n",
       "  'investig',\n",
       "  'june',\n",
       "  'talent',\n",
       "  'violenc',\n",
       "  'via',\n",
       "  'tv'],\n",
       " ['now',\n",
       "  'week',\n",
       "  'someth',\n",
       "  'beat',\n",
       "  'side',\n",
       "  'dez',\n",
       "  'cup',\n",
       "  'fumbl',\n",
       "  'half',\n",
       "  'wednesday',\n",
       "  'cavali',\n",
       "  'nonenon',\n",
       "  'hay',\n",
       "  'yet',\n",
       "  'lap',\n",
       "  'kind',\n",
       "  'earlier',\n",
       "  'wait',\n",
       "  'someon',\n",
       "  'pittsburgh',\n",
       "  'set',\n",
       "  'head',\n",
       "  'shiffrin',\n",
       "  'uhow',\n",
       "  'shoulder',\n",
       "  'owner',\n",
       "  'ucit',\n",
       "  'dan',\n",
       "  'england',\n",
       "  'vintag',\n",
       "  'catch',\n",
       "  'worst',\n",
       "  'loyalti',\n",
       "  'climb',\n",
       "  'east',\n",
       "  'activ',\n",
       "  'destroy',\n",
       "  'search',\n",
       "  'januari',\n",
       "  'seahawk',\n",
       "  'uwhi',\n",
       "  'certain',\n",
       "  'ricki',\n",
       "  'clariti',\n",
       "  'whether',\n",
       "  'domest',\n",
       "  'transfer',\n",
       "  'greg',\n",
       "  'warrior',\n",
       "  'increas'],\n",
       " ['back',\n",
       "  'career',\n",
       "  'sport',\n",
       "  'day',\n",
       "  'organ',\n",
       "  'pennsylvania',\n",
       "  'club',\n",
       "  'took',\n",
       "  'tuesday',\n",
       "  'total',\n",
       "  'ask',\n",
       "  'voter',\n",
       "  'uium',\n",
       "  'white',\n",
       "  'almost',\n",
       "  'gut',\n",
       "  'jerri',\n",
       "  'whole',\n",
       "  'invit',\n",
       "  'rule',\n",
       "  'sit',\n",
       "  'without',\n",
       "  'chief',\n",
       "  'leav',\n",
       "  'high',\n",
       "  'miracl',\n",
       "  'especi',\n",
       "  'hendrick',\n",
       "  'announc',\n",
       "  'smith',\n",
       "  'notch',\n",
       "  'bournemouth',\n",
       "  'montreal',\n",
       "  'silver',\n",
       "  'trio',\n",
       "  'awardudxah',\n",
       "  'sestrier',\n",
       "  'cizeronu',\n",
       "  'email',\n",
       "  'legal',\n",
       "  'attack',\n",
       "  'disrespect',\n",
       "  'tweet',\n",
       "  'auditthevot',\n",
       "  'uheu',\n",
       "  'tim',\n",
       "  'favorit',\n",
       "  'stronger',\n",
       "  'self',\n",
       "  'ueven'],\n",
       " ['play',\n",
       "  'itu',\n",
       "  'point',\n",
       "  'lead',\n",
       "  'sign',\n",
       "  'work',\n",
       "  'sunday',\n",
       "  'uit',\n",
       "  'far',\n",
       "  'watch',\n",
       "  'abl',\n",
       "  'pitch',\n",
       "  'old',\n",
       "  'happi',\n",
       "  'seen',\n",
       "  'grand',\n",
       "  'allow',\n",
       "  'past',\n",
       "  'bad',\n",
       "  'hillari',\n",
       "  'read',\n",
       "  'month',\n",
       "  'chanc',\n",
       "  'fun',\n",
       "  'given',\n",
       "  'near',\n",
       "  'qualiti',\n",
       "  'arenut',\n",
       "  'found',\n",
       "  'earn',\n",
       "  'uthey',\n",
       "  'forget',\n",
       "  'knew',\n",
       "  'fulfil',\n",
       "  'contest',\n",
       "  'begin',\n",
       "  'process',\n",
       "  'againud',\n",
       "  'ranger',\n",
       "  'round',\n",
       "  'touch',\n",
       "  'crazi',\n",
       "  'bother',\n",
       "  'dollar',\n",
       "  'exampl',\n",
       "  'enter',\n",
       "  'miami',\n",
       "  'fever',\n",
       "  'hair',\n",
       "  'fear'],\n",
       " ['even',\n",
       "  'elect',\n",
       "  'us',\n",
       "  'best',\n",
       "  'contract',\n",
       "  'hope',\n",
       "  'person',\n",
       "  'decid',\n",
       "  'three',\n",
       "  'giant',\n",
       "  'seri',\n",
       "  'statement',\n",
       "  'mix',\n",
       "  'honor',\n",
       "  'surpris',\n",
       "  'hurt',\n",
       "  'full',\n",
       "  'danc',\n",
       "  'cover',\n",
       "  'uour',\n",
       "  'margin',\n",
       "  'outsid',\n",
       "  'case',\n",
       "  'agenc',\n",
       "  'accord',\n",
       "  'injur',\n",
       "  'limit',\n",
       "  'avoid',\n",
       "  'skill',\n",
       "  'st',\n",
       "  'respond',\n",
       "  'uwe',\n",
       "  'skin',\n",
       "  'treat',\n",
       "  'tough',\n",
       "  'theyr',\n",
       "  'realiz',\n",
       "  'scale',\n",
       "  'campaignafterthecampaign',\n",
       "  'spokeswoman',\n",
       "  'suppos',\n",
       "  'simpli',\n",
       "  'resultsm',\n",
       "  'perfectli',\n",
       "  'retireud',\n",
       "  'ainut',\n",
       "  'voic',\n",
       "  'south',\n",
       "  'vs',\n",
       "  'clipud'],\n",
       " ['can',\n",
       "  'much',\n",
       "  'also',\n",
       "  'ua',\n",
       "  'lost',\n",
       "  'democrat',\n",
       "  'finish',\n",
       "  'inform',\n",
       "  'save',\n",
       "  'check',\n",
       "  'romo',\n",
       "  'bottom',\n",
       "  'sidelin',\n",
       "  'steeler',\n",
       "  'er',\n",
       "  'opinion',\n",
       "  'other',\n",
       "  'publish',\n",
       "  'feb',\n",
       "  'notwithstand',\n",
       "  'plan',\n",
       "  'profession',\n",
       "  'small',\n",
       "  'exclus',\n",
       "  'onlin',\n",
       "  'convers',\n",
       "  'uearnhardt',\n",
       "  'athlet',\n",
       "  'low',\n",
       "  'funni',\n",
       "  'relat',\n",
       "  'iron',\n",
       "  'vega',\n",
       "  'hockey',\n",
       "  'address',\n",
       "  'michel',\n",
       "  'write',\n",
       "  'echo',\n",
       "  'ms',\n",
       "  'shrunk',\n",
       "  'precinct',\n",
       "  'foot',\n",
       "  'staffth',\n",
       "  'fast',\n",
       "  'subject',\n",
       "  'faith',\n",
       "  'innocu',\n",
       "  'climat',\n",
       "  'armi',\n",
       "  'bush'],\n",
       " ['make',\n",
       "  'tri',\n",
       "  'mr',\n",
       "  'score',\n",
       "  'lose',\n",
       "  'polic',\n",
       "  'done',\n",
       "  'problem',\n",
       "  'qb',\n",
       "  'power',\n",
       "  'johnson',\n",
       "  'uucw',\n",
       "  'video',\n",
       "  'orang',\n",
       "  'jone',\n",
       "  'driver',\n",
       "  'dozen',\n",
       "  'draft',\n",
       "  'parti',\n",
       "  'commit',\n",
       "  'critic',\n",
       "  'transit',\n",
       "  'vike',\n",
       "  'certifi',\n",
       "  'ab',\n",
       "  'govern',\n",
       "  'saturday',\n",
       "  'affili',\n",
       "  'blotter',\n",
       "  'havenut',\n",
       "  'itud',\n",
       "  'uthough',\n",
       "  'stadium',\n",
       "  'kneel',\n",
       "  'benchmark',\n",
       "  'shiffrinxahop',\n",
       "  'ushiffrin',\n",
       "  'emot',\n",
       "  'tonight',\n",
       "  'mind',\n",
       "  'innoc',\n",
       "  'grow',\n",
       "  'buy',\n",
       "  'influenc',\n",
       "  'commission',\n",
       "  'nflu',\n",
       "  'narrow',\n",
       "  'harbor',\n",
       "  'charg',\n",
       "  'welcom'],\n",
       " ['will',\n",
       "  'didnut',\n",
       "  'earnhardt',\n",
       "  'canut',\n",
       "  'pitcher',\n",
       "  'ballot',\n",
       "  'titl',\n",
       "  'thank',\n",
       "  'nice',\n",
       "  'wonut',\n",
       "  'ucth',\n",
       "  'sever',\n",
       "  'youur',\n",
       "  'pressur',\n",
       "  'continu',\n",
       "  'excit',\n",
       "  'today',\n",
       "  'fight',\n",
       "  'cross',\n",
       "  'brotherud',\n",
       "  'knick',\n",
       "  'mistak',\n",
       "  'shark',\n",
       "  'jr',\n",
       "  'shock',\n",
       "  'scoreboard',\n",
       "  'toxawin',\n",
       "  'ushiffrinxaappear',\n",
       "  'decemb',\n",
       "  'experi',\n",
       "  'bathroom',\n",
       "  'written',\n",
       "  'space',\n",
       "  'extens',\n",
       "  'institut',\n",
       "  'basic',\n",
       "  'tallahasse',\n",
       "  'certif',\n",
       "  'promin',\n",
       "  'manich',\n",
       "  'pore',\n",
       "  'trumpu',\n",
       "  'mediabut',\n",
       "  'uhi',\n",
       "  'per',\n",
       "  'root',\n",
       "  'exercis',\n",
       "  'twist',\n",
       "  'overcom',\n",
       "  'injustic'],\n",
       " ['go',\n",
       "  'nfl',\n",
       "  'final',\n",
       "  'great',\n",
       "  'alway',\n",
       "  'famili',\n",
       "  'name',\n",
       "  'campaign',\n",
       "  'perfect',\n",
       "  'close',\n",
       "  'step',\n",
       "  'uyou',\n",
       "  'trash',\n",
       "  'hack',\n",
       "  'word',\n",
       "  'donald',\n",
       "  'strength',\n",
       "  'leadership',\n",
       "  'sixtim',\n",
       "  'happygolucki',\n",
       "  'requir',\n",
       "  'ucbut',\n",
       "  'sometim',\n",
       "  'calendar',\n",
       "  'gs',\n",
       "  'redskin',\n",
       "  'event',\n",
       "  'ram',\n",
       "  'johnsonu',\n",
       "  'ben',\n",
       "  'quickli',\n",
       "  'ref',\n",
       "  'internet',\n",
       "  'eagl',\n",
       "  'gestur',\n",
       "  'oppress',\n",
       "  'john',\n",
       "  'shift',\n",
       "  'prescott',\n",
       "  'seat',\n",
       "  'sophist',\n",
       "  'helen',\n",
       "  'recountmr',\n",
       "  'votethat',\n",
       "  'mondaymichigan',\n",
       "  'bs',\n",
       "  'demor',\n",
       "  'venezuelanon',\n",
       "  'project',\n",
       "  'statesponsor'],\n",
       " ['look',\n",
       "  'night',\n",
       "  'million',\n",
       "  'result',\n",
       "  'recount',\n",
       "  'mean',\n",
       "  'hit',\n",
       "  'machin',\n",
       "  'differ',\n",
       "  'anyon',\n",
       "  'product',\n",
       "  'la',\n",
       "  'break',\n",
       "  'speak',\n",
       "  'uon',\n",
       "  'later',\n",
       "  'unless',\n",
       "  'receiv',\n",
       "  'togeth',\n",
       "  'spring',\n",
       "  'request',\n",
       "  'quick',\n",
       "  'bullet',\n",
       "  'stori',\n",
       "  'erv',\n",
       "  'wrote',\n",
       "  'independ',\n",
       "  'host',\n",
       "  'approach',\n",
       "  'often',\n",
       "  'indian',\n",
       "  'suspend',\n",
       "  'clearli',\n",
       "  'fail',\n",
       "  'td',\n",
       "  'impact',\n",
       "  'road',\n",
       "  'hundr',\n",
       "  'whatev',\n",
       "  'loss',\n",
       "  'folk',\n",
       "  'argument',\n",
       "  'suddenli',\n",
       "  'extrem',\n",
       "  'explain',\n",
       "  'park',\n",
       "  'credit',\n",
       "  'ok',\n",
       "  'wi',\n",
       "  'riggedbas'],\n",
       " ['last',\n",
       "  'win',\n",
       "  'state',\n",
       "  'clinton',\n",
       "  'everi',\n",
       "  'pass',\n",
       "  'talk',\n",
       "  'twin',\n",
       "  'support',\n",
       "  'ubut',\n",
       "  'feel',\n",
       "  'brown',\n",
       "  'long',\n",
       "  'follow',\n",
       "  'love',\n",
       "  'major',\n",
       "  'offer',\n",
       "  'earli',\n",
       "  'american',\n",
       "  'nearli',\n",
       "  'uafter',\n",
       "  'stay',\n",
       "  'duck',\n",
       "  'data',\n",
       "  'king',\n",
       "  'sourc',\n",
       "  'felt',\n",
       "  'program',\n",
       "  'tragic',\n",
       "  'group',\n",
       "  'aba',\n",
       "  'kill',\n",
       "  'claim',\n",
       "  'throw',\n",
       "  'taken',\n",
       "  'view',\n",
       "  'book',\n",
       "  'domin',\n",
       "  'dad',\n",
       "  'wish',\n",
       "  'similar',\n",
       "  'six',\n",
       "  'exactli',\n",
       "  'admit',\n",
       "  'conduct',\n",
       "  'unow',\n",
       "  'victim',\n",
       "  'volunt',\n",
       "  'confid',\n",
       "  'member'],\n",
       " ['vote',\n",
       "  'season',\n",
       "  'want',\n",
       "  'say',\n",
       "  'think',\n",
       "  'first',\n",
       "  'world',\n",
       "  'thing',\n",
       "  'anoth',\n",
       "  'littl',\n",
       "  'alreadi',\n",
       "  'reason',\n",
       "  'social',\n",
       "  'black',\n",
       "  'cub',\n",
       "  'olymp',\n",
       "  'wisconsin',\n",
       "  'isnut',\n",
       "  'patriot',\n",
       "  'import',\n",
       "  'expect',\n",
       "  'inning',\n",
       "  'san',\n",
       "  'dave',\n",
       "  'sacramento',\n",
       "  'uvia',\n",
       "  'santa',\n",
       "  'righthand',\n",
       "  'teammat',\n",
       "  'weeku',\n",
       "  'els',\n",
       "  'bunch',\n",
       "  'secur',\n",
       "  'slap',\n",
       "  'law',\n",
       "  'demand',\n",
       "  'solo',\n",
       "  'goodby',\n",
       "  'breakthrough',\n",
       "  'cultur',\n",
       "  'urg',\n",
       "  'across',\n",
       "  'infect',\n",
       "  'slip',\n",
       "  'patrick',\n",
       "  'uthank',\n",
       "  'philadelphia',\n",
       "  'databas',\n",
       "  'enforc',\n",
       "  'slice'],\n",
       " ['good',\n",
       "  'run',\n",
       "  'nation',\n",
       "  'footbal',\n",
       "  'ium',\n",
       "  'playoff',\n",
       "  'sinc',\n",
       "  'dalla',\n",
       "  'matter',\n",
       "  'least',\n",
       "  'uwhen',\n",
       "  'friday',\n",
       "  'meet',\n",
       "  'agre',\n",
       "  'learn',\n",
       "  'uso',\n",
       "  'counti',\n",
       "  'success',\n",
       "  'huge',\n",
       "  'ignor',\n",
       "  'goe',\n",
       "  'alltim',\n",
       "  'sens',\n",
       "  'bill',\n",
       "  'intro',\n",
       "  'effort',\n",
       "  'rodger',\n",
       "  'decis',\n",
       "  'yearu',\n",
       "  'becam',\n",
       "  'key',\n",
       "  'room',\n",
       "  'trophi',\n",
       "  'penguin',\n",
       "  'nine',\n",
       "  'zeke',\n",
       "  'spider',\n",
       "  'stupid',\n",
       "  'confus',\n",
       "  'blame',\n",
       "  'cede',\n",
       "  'itxath',\n",
       "  'tear',\n",
       "  'punk',\n",
       "  'singl',\n",
       "  'punish',\n",
       "  'militari',\n",
       "  'hero',\n",
       "  'howev',\n",
       "  'magazin'],\n",
       " ['peopl',\n",
       "  'thatu',\n",
       "  'heu',\n",
       "  'moir',\n",
       "  'concuss',\n",
       "  'media',\n",
       "  'five',\n",
       "  'form',\n",
       "  'seem',\n",
       "  'though',\n",
       "  'compet',\n",
       "  'offens',\n",
       "  'end',\n",
       "  'unit',\n",
       "  'weur',\n",
       "  'face',\n",
       "  'forward',\n",
       "  'cizeron',\n",
       "  'nhl',\n",
       "  'interest',\n",
       "  'current',\n",
       "  'ucthat',\n",
       "  'incred',\n",
       "  'ucclassicud',\n",
       "  'higha',\n",
       "  'lynch',\n",
       "  'shoe',\n",
       "  'speedway',\n",
       "  'court',\n",
       "  'venu',\n",
       "  'downhil',\n",
       "  'workload',\n",
       "  'physic',\n",
       "  'werenut',\n",
       "  'wide',\n",
       "  'oblig',\n",
       "  'revers',\n",
       "  'seek',\n",
       "  'battl',\n",
       "  'kept',\n",
       "  'secret',\n",
       "  'expens',\n",
       "  'term',\n",
       "  'con',\n",
       "  'worth',\n",
       "  'plead',\n",
       "  'zuckermanpark',\n",
       "  'certifiedunit',\n",
       "  'review',\n",
       "  'garbag'],\n",
       " ['trump',\n",
       "  'cowboy',\n",
       "  'uif',\n",
       "  'may',\n",
       "  'champion',\n",
       "  'lebron',\n",
       "  'move',\n",
       "  'super',\n",
       "  'polit',\n",
       "  'probabl',\n",
       "  'anthem',\n",
       "  'money',\n",
       "  'tell',\n",
       "  'iuv',\n",
       "  'multipl',\n",
       "  'remain',\n",
       "  'forc',\n",
       "  'pro',\n",
       "  'type',\n",
       "  'noah',\n",
       "  'racist',\n",
       "  'myer',\n",
       "  'color',\n",
       "  'winner',\n",
       "  'track',\n",
       "  'opportun',\n",
       "  'harder',\n",
       "  'laugh',\n",
       "  'fire',\n",
       "  'cool',\n",
       "  'idea',\n",
       "  'flood',\n",
       "  'ucaft',\n",
       "  'concern',\n",
       "  'seriou',\n",
       "  'stuff',\n",
       "  'roll',\n",
       "  'zero',\n",
       "  'unew',\n",
       "  'ten',\n",
       "  'contact',\n",
       "  'difficult',\n",
       "  'd',\n",
       "  'deficit',\n",
       "  'arm',\n",
       "  'wors',\n",
       "  'percentag',\n",
       "  'intens',\n",
       "  'anthoni',\n",
       "  'declin']]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "website_list = [\"animalplanet\",\"natgeo\",\"Discovery\",\"TMZ\",\"HISTORY\",\"NBCSports\",\"ESPN\",\"nytimes\",\"cnn\",\"bbc\",\"theatlantic\"]\n",
    "final = pd.DataFrame({\"Topics\":np.arange(30)})\n",
    "for website in website_list:\n",
    "    text = sc.textFile(\"C:/Users/Abhinav/Google Drive/Big Data Project/Datasets/Website Scraped/\"+website+\"_top10_posts_BOW_*\")\n",
    "    text = text.map(removePunctuation)\n",
    "    text = text.map(removeStopWords)\n",
    "    text = text.map(stemmingWords)\n",
    "\n",
    "    word_list_1 = text.collect()[0]\n",
    "\n",
    "    #doc_clean = [doc.split() for doc in word_list]\n",
    "    #dictionary = corpora.Dictionary(doc_clean)\n",
    "    #corpus = [dictionary.doc2bow(text) for text in doc_clean]\n",
    "\n",
    "    doc_clean_1 = [doc.split() for doc in word_list_1]\n",
    "    #dictionary_1 = corpora.Dictionary(doc_clean_1)\n",
    "    corpus_1 = [dictionary.doc2bow(text) for text in doc_clean_1]\n",
    "\n",
    "\n",
    "    #doc_clean_1 = [doc.split() for doc in word_list_1]\n",
    "    vec_bow = dictionary.doc2bow(word_list_1)\n",
    "    vec_lsi = ldamodel[vec_bow] # convert the query to LSI space\n",
    "    vec_lsi\n",
    "\n",
    "    output = pd.DataFrame(vec_lsi)\n",
    "    output.columns = [\"Topics\", website]\n",
    "\n",
    "    final = final.merge(output)\n",
    "    final.to_csv(\"topic_probability_clustering.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from math import sqrt\n",
    "\n",
    "from pyspark.mllib.clustering import KMeans, KMeansModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 0.026723  ,  0.02823678,  0.03274008,  0.0498035 ,  0.03433012,\n",
       "         0.05815009,  0.03493342,  0.03341556,  0.02631256,  0.03590577,\n",
       "         0.03022529,  0.0221985 ,  0.01878709,  0.01007155,  0.0252264 ,\n",
       "         0.02684838,  0.02098522,  0.0371751 ]),\n",
       " array([ 0.04294931,  0.03910734,  0.02518367,  0.02439072,  0.02551375,\n",
       "         0.03770163,  0.03053104,  0.03304557,  0.03098319,  0.02442024,\n",
       "         0.03012939,  0.02747059,  0.03280636,  0.03814229,  0.05446749,\n",
       "         0.03616599,  0.0244186 ,  0.02594528]),\n",
       " array([ 0.01909358,  0.01454822,  0.03007739,  0.01433485,  0.02587966,\n",
       "         0.01481093,  0.05056253,  0.02212454,  0.02091468,  0.01557134,\n",
       "         0.0206006 ,  0.02446442,  0.03295419,  0.03745415,  0.02287935,\n",
       "         0.03567992,  0.01560564,  0.04768854]),\n",
       " array([ 0.02255615,  0.04329312,  0.01142147,  0.02494486,  0.03707364,\n",
       "         0.01840014,  0.03900605,  0.03236733,  0.03451242,  0.05260936,\n",
       "         0.02295111,  0.04821898,  0.02060314,  0.03840716,  0.01571349,\n",
       "         0.05032243,  0.0114515 ,  0.02290961]),\n",
       " array([ 0.0294529 ,  0.03191327,  0.04275288,  0.01562436,  0.03498295,\n",
       "         0.02466314,  0.03605082,  0.04414234,  0.03281331,  0.04349967,\n",
       "         0.0223378 ,  0.02852112,  0.03762974,  0.05218153,  0.02246533,\n",
       "         0.04009787,  0.03084604,  0.04038085])]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load and parse the data\n",
    "data = sc.textFile(\"C:/Users/Abhinav/Google Drive/Big Data Project/Codes/clustering_probability.csv\")\n",
    "parsedData = data.map(lambda line: array([float(x) for x in line.split(',')]))\n",
    "parsedData.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 0.026723  ,  0.02823678,  0.03274008,  0.0498035 ,  0.03433012,\n",
       "         0.05815009,  0.03493342,  0.03341556,  0.02631256,  0.03590577,\n",
       "         0.03022529,  0.0221985 ,  0.01878709,  0.01007155,  0.0252264 ,\n",
       "         0.02684838,  0.02098522,  0.0371751 ]),\n",
       " array([ 0.04294931,  0.03910734,  0.02518367,  0.02439072,  0.02551375,\n",
       "         0.03770163,  0.03053104,  0.03304557,  0.03098319,  0.02442024,\n",
       "         0.03012939,  0.02747059,  0.03280636,  0.03814229,  0.05446749,\n",
       "         0.03616599,  0.0244186 ,  0.02594528]),\n",
       " array([ 0.01909358,  0.01454822,  0.03007739,  0.01433485,  0.02587966,\n",
       "         0.01481093,  0.05056253,  0.02212454,  0.02091468,  0.01557134,\n",
       "         0.0206006 ,  0.02446442,  0.03295419,  0.03745415,  0.02287935,\n",
       "         0.03567992,  0.01560564,  0.04768854]),\n",
       " array([ 0.02255615,  0.04329312,  0.01142147,  0.02494486,  0.03707364,\n",
       "         0.01840014,  0.03900605,  0.03236733,  0.03451242,  0.05260936,\n",
       "         0.02295111,  0.04821898,  0.02060314,  0.03840716,  0.01571349,\n",
       "         0.05032243,  0.0114515 ,  0.02290961]),\n",
       " array([ 0.0294529 ,  0.03191327,  0.04275288,  0.01562436,  0.03498295,\n",
       "         0.02466314,  0.03605082,  0.04414234,  0.03281331,  0.04349967,\n",
       "         0.0223378 ,  0.02852112,  0.03762974,  0.05218153,  0.02246533,\n",
       "         0.04009787,  0.03084604,  0.04038085]),\n",
       " array([ 0.04023677,  0.03676157,  0.03229981,  0.03152762,  0.02630748,\n",
       "         0.03302023,  0.02958253,  0.03283216,  0.02944431,  0.03628306,\n",
       "         0.02973054,  0.03297419,  0.02561047,  0.03327231,  0.0458503 ,\n",
       "         0.03486108,  0.03555538,  0.0370006 ]),\n",
       " array([ 0.03222045,  0.04118483,  0.02502697,  0.0470347 ,  0.02190892,\n",
       "         0.02268673,  0.0199363 ,  0.03487252,  0.02681475,  0.02988978,\n",
       "         0.02413561,  0.03863355,  0.03119778,  0.03880149,  0.04705844,\n",
       "         0.0357354 ,  0.02817384,  0.02761497]),\n",
       " array([ 0.02830866,  0.03638754,  0.03369267,  0.02371436,  0.01811609,\n",
       "         0.0348773 ,  0.02555959,  0.01918489,  0.01987311,  0.03454089,\n",
       "         0.03092349,  0.03379847,  0.05559458,  0.06733962,  0.05926424,\n",
       "         0.03532357,  0.03611906,  0.03992016]),\n",
       " array([ 0.03522263,  0.01072243,  0.02803375,  0.06828724,  0.02171271,\n",
       "         0.02581155,  0.01103565,  0.0470307 ,  0.05105578,  0.01770206,\n",
       "         0.01468797,  0.1032367 ,  0.01849585,  0.02388313,  0.03985698,\n",
       "         0.07482899,  0.12395207,  0.01086357]),\n",
       " array([ 0.03525825,  0.01072243,  0.02816796,  0.06827522,  0.02177689,\n",
       "         0.02562192,  0.01099998,  0.04706559,  0.05095616,  0.01772318,\n",
       "         0.01448898,  0.10309945,  0.01866629,  0.02385047,  0.03978827,\n",
       "         0.07472844,  0.12372947,  0.01093328]),\n",
       " array([ 0.03279771,  0.02879599,  0.03166046,  0.02191115,  0.02703949,\n",
       "         0.03364486,  0.04314388,  0.03499793,  0.02697571,  0.02745422,\n",
       "         0.02548797,  0.03012962,  0.03061399,  0.05257213,  0.04483245,\n",
       "         0.03239331,  0.03787849,  0.02463383])]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsedData.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:/Users/Abhinav/Downloads/spark-2.0.2-bin-hadoop2.7/python\\pyspark\\mllib\\clustering.py:347: UserWarning: The param `runs` has no effect since Spark 2.0.0.\n",
      "  warnings.warn(\"The param `runs` has no effect since Spark 2.0.0.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Within Set Sum of Squared Error = 0.153241988881\n"
     ]
    }
   ],
   "source": [
    "# Build the model (cluster the data)\n",
    "clusters = KMeans.train(parsedData, 7, maxIterations=10,\n",
    "                        runs=10, initializationMode=\"random\", seed = 1234)\n",
    "\n",
    "# Evaluate clustering by computing Within Set Sum of Squared Errors\n",
    "def error(point):\n",
    "    center = clusters.centers[clusters.predict(point)]\n",
    "    return sqrt(sum([x**2 for x in (point - center)]))\n",
    "\n",
    "WSSSE = parsedData.map(lambda point: error(point)).reduce(lambda x, y: x + y)\n",
    "print(\"Within Set Sum of Squared Error = \" + str(WSSSE))\n",
    "\n",
    "# Save and load model\n",
    "#clusters.save(sc, \"target/org/apache/spark/PythonKMeansExample/KMeansModel\")\n",
    "#sameModel = KMeansModel.load(sc, \"target/org/apache/spark/PythonKMeansExample/KMeansModel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4]\n",
      "[0, 1, 2, 3, 4]\n",
      "[0, 1, 2, 3, 4]\n",
      "[0, 1, 2, 3, 4]\n",
      "[0, 1, 2, 3, 4]\n",
      "[0, 1, 2, 3, 4]\n",
      "[0, 1, 2, 3, 4]\n",
      "[0, 1, 2, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,8):\n",
    "    list_i = list()\n",
    "    for j in range(0,5):\n",
    "        list_i.append(j)\n",
    "    print list_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14891262255219834"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(sum(np.square(parsedData.collect()[1]-clusters.clusterCenters[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.062655952452143088, 0.035623685735485966, 0.068981238901420008, 0.060558839847077252, 0.057060871363185488, 0.030302515466788991, 0.0, 0.054868605109333106, 0.13435648465484759, 0.13408264994666647, 0.043626709584165521]\n",
      "[0.15627587002438359, 0.14891262255219834, 0.16711167291195816, 0.15301961130377109, 0.15197184288774399, 0.13748400751901571, 0.13421942278980323, 0.15450721388196956, 0.00023987233344843772, 0.00023987233344844477, 0.14233062643050789]\n",
      "[0.062655952452143088, 0.035623685735485966, 0.068981238901420008, 0.060558839847077252, 0.057060871363185488, 0.030302515466788991, 0.0, 0.054868605109333106, 0.13435648465484759, 0.13408264994666647, 0.043626709584165521]\n",
      "[0.064449714428961372, 0.020551255414960209, 0.05921210545593715, 0.066106101970974246, 0.043934878850099675, 0.022489341818718103, 0.033604276711877976, 0.035259596871661157, 0.14384214983383559, 0.14360058576894991, 0.020317224843583281]\n",
      "[0.075763559771385408, 0.067225776757201886, 0.069659943355994236, 0.0, 0.056762584408761627, 0.061660775624630182, 0.060558839847077252, 0.086354077399505791, 0.15313996351511, 0.1528995406768047, 0.065466306355974113]\n",
      "[0.0, 0.062012550135778971, 0.074471708563434977, 0.075763559771385408, 0.071754092541182035, 0.052224074525757933, 0.062655952452143088, 0.090139719772107343, 0.15639039716259348, 0.15616162734985523, 0.067090511522547497]\n",
      "[0.067929574271037046, 0.053273726457219339, 0.027072412632392358, 0.057483335174836024, 0.027072412632392354, 0.04741244919168365, 0.057221164395806157, 0.06187349395133681, 0.157553183401795, 0.15726738593900022, 0.040135094243940281]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-47-a8869f936ef0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mlist_i\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m11\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m         \u001b[0mlist_i\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparsedData\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mclusters\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclusterCenters\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[1;32mprint\u001b[0m \u001b[0mlist_i\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "for i in range(0,7):\n",
    "    list_i = list()\n",
    "    for j in range(0,11):\n",
    "        list_i.append(np.sqrt(sum(np.square(parsedData.collect()[j]-clusters.clusterCenters[i]))))\n",
    "    print list_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 0.03222045,  0.04118483,  0.02502697,  0.0470347 ,  0.02190892,\n",
       "         0.02268673,  0.0199363 ,  0.03487252,  0.02681475,  0.02988978,\n",
       "         0.02413561,  0.03863355,  0.03119778,  0.03880149,  0.04705844,\n",
       "         0.0357354 ,  0.02817384,  0.02761497]),\n",
       " array([ 0.03524044,  0.01072243,  0.02810085,  0.06828123,  0.0217448 ,\n",
       "         0.02571673,  0.01101781,  0.04704815,  0.05100597,  0.01771262,\n",
       "         0.01458848,  0.10316807,  0.01858107,  0.0238668 ,  0.03982263,\n",
       "         0.07477872,  0.12384077,  0.01089842]),\n",
       " array([ 0.03222045,  0.04118483,  0.02502697,  0.0470347 ,  0.02190892,\n",
       "         0.02268673,  0.0199363 ,  0.03487252,  0.02681475,  0.02988978,\n",
       "         0.02413561,  0.03863355,  0.03119778,  0.03880149,  0.04705844,\n",
       "         0.0357354 ,  0.02817384,  0.02761497]),\n",
       " array([ 0.03607311,  0.03526311,  0.03070915,  0.02538596,  0.0242442 ,\n",
       "         0.034811  ,  0.03220426,  0.03001514,  0.02681908,  0.0306746 ,\n",
       "         0.02906785,  0.03109322,  0.03615635,  0.04783159,  0.05110362,\n",
       "         0.03468599,  0.03349288,  0.03187497]),\n",
       " array([ 0.02255615,  0.04329312,  0.01142147,  0.02494486,  0.03707364,\n",
       "         0.01840014,  0.03900605,  0.03236733,  0.03451242,  0.05260936,\n",
       "         0.02295111,  0.04821898,  0.02060314,  0.03840716,  0.01571349,\n",
       "         0.05032243,  0.0114515 ,  0.02290961]),\n",
       " array([ 0.026723  ,  0.02823678,  0.03274008,  0.0498035 ,  0.03433012,\n",
       "         0.05815009,  0.03493342,  0.03341556,  0.02631256,  0.03590577,\n",
       "         0.03022529,  0.0221985 ,  0.01878709,  0.01007155,  0.0252264 ,\n",
       "         0.02684838,  0.02098522,  0.0371751 ]),\n",
       " array([ 0.02427324,  0.02323075,  0.03641513,  0.01497961,  0.03043131,\n",
       "         0.01973704,  0.04330668,  0.03313344,  0.026864  ,  0.0295355 ,\n",
       "         0.0214692 ,  0.02649277,  0.03529197,  0.04481784,  0.02267234,\n",
       "         0.0378889 ,  0.02322584,  0.04403469])]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clusters.centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
